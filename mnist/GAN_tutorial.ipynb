{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_tutorial.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1pwFfKhWDMMdQLWqQ1bqgmXpOFBR13SOY","authorship_tag":"ABX9TyOHPrjK1H2es5lbhSzPBi1J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ihwXaJDXngf-","colab_type":"text"},"source":["# GAN (Generative Adversarial Network)\n","- Generative\n","  - 생성모델 => 가짜를 만들어내는 모델\n","\n","    <img src=\"https://files.slack.com/files-pri/T25783BPY-F9RKVJ9TP/_______________1.png?pub_secret=4674d81e77\" width=\"450px\" height=\"300px\">\n","\n","- Advesarial  \n","  - 두 개의 모델을 적대적(Adversarial)으로 경쟁시키며 발전시킨다는 것"]},{"cell_type":"markdown","metadata":{"id":"rBQM_BLEr-r0","colab_type":"text"},"source":["<img src=\"https://files.slack.com/files-pri/T25783BPY-F9SHTP6F9/picture2.png?pub_secret=6821873e68\" width=\"450px\" height=\"300px\">\n","\n","GAN은 위조지폐범에 해당하는 생성자(Generator)와 경찰에 해당하는 구분자(Discriminator)를 경쟁적으로 학습시킨다.  \n","생성자의 목적은 그럴듯한 가짜 데이터를 만들어서 구분자를 속이는 것이며,   \n","구분자의 목적은 생성자가 만든 가짜 데이터와 진짜 데이터를 구분하는 것이다.  \n","시간이 흐르면 위조지폐범의 위폐 제조 기술은 완벽에 가깝게 발전할 것이다.\n"]},{"cell_type":"markdown","metadata":{"id":"0QE72hqtsrmD","colab_type":"text"},"source":["## 실행 코드"]},{"cell_type":"markdown","metadata":{"id":"lYzDYz7aszh9","colab_type":"text"},"source":["### 0. import library"]},{"cell_type":"code","metadata":{"id":"VNIJdkwkr-Xs","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torch.optim import Adam\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"544B_o9cyw8L","colab_type":"text"},"source":["#### GPU check"]},{"cell_type":"code","metadata":{"id":"ZPf31uinyxTK","colab_type":"code","colab":{}},"source":["import os\n","import imageio\n","\n","if torch.cuda.is_available():\n","    use_gpu = True\n","leave_log = True\n","if leave_log:\n","    result_dir = '/content/drive/My Drive/Colab deeplearning/GAN_generated_images'\n","    if not os.path.isdir(result_dir):\n","        os.mkdir(result_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"171OZZV7yxRK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"leIUnb9is-cN","colab_type":"text"},"source":["### 1. 데이터 로드 & 전처리 방식 지정"]},{"cell_type":"code","metadata":{"id":"r61HKPNInd5B","colab_type":"code","colab":{}},"source":["#데이터 전처리 방식을 지정한다.\n","transform = transforms.Compose([\n","  transforms.ToTensor(), # 데이터를 파이토치의 Tensor 형식으로바꾼다.\n","  transforms.Normalize(mean=(0.5,), std=(0.5,)) # 픽셀값 0 ~ 1 -> -1 ~ 1\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyKHJGytnd09","colab_type":"code","colab":{}},"source":["#MNIST 데이터셋을 불러온다. 지정한 폴더에 없을 경우 자동으로 다운로드한다.\n","dataPath = \"/content/drive/My Drive/Colab deeplearning/MNIST\"\n","mnist =datasets.MNIST(root='/content/drive/My Drive/Colab deeplearning/mnist', download=True, transform=transform)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-FW9lxxnd3D","colab_type":"code","colab":{}},"source":["#데이터를 한번에 batch_size만큼만 가져오는 dataloader를 만든다.\n","dataloader =DataLoader(mnist, batch_size=60, shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A83fhO8gVQYv","colab_type":"code","outputId":"5cbe3724-53f7-44af-bf12-71e78cc94f4a","executionInfo":{"status":"ok","timestamp":1589251018002,"user_tz":-540,"elapsed":7103,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type(mnist)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torchvision.datasets.mnist.MNIST"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"UPFH07LdPveq","colab_type":"code","outputId":"fbe61621-b857-4c2a-f0e9-038fdba36480","executionInfo":{"status":"ok","timestamp":1589251018002,"user_tz":-540,"elapsed":6355,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["mnist[0][0].shape"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 28, 28])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"H6tQt_2IPvb7","colab_type":"code","outputId":"c888e662-a973-4401-a96d-69de89999cbc","executionInfo":{"status":"ok","timestamp":1589251018003,"user_tz":-540,"elapsed":5931,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["mnist[0][0][0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -0.8588, -0.8588, -0.8588,\n","         -0.0118,  0.0667,  0.3725, -0.7961,  0.3020,  1.0000,  0.9373, -0.0039,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,  0.9843,  0.9843,\n","          0.9843,  0.9843,  0.7647,  0.3490,  0.9843,  0.8980,  0.5294, -0.4980,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6157,\n","          0.8667,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n","          0.9843,  0.9686, -0.2706, -0.3569, -0.3569, -0.5608, -0.6941, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n","          0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,  0.5529,  0.4275,\n","          0.9373,  0.8902, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -0.3725,  0.2235, -0.1608,  0.9843,  0.9843,  0.6078, -0.9137, -1.0000,\n","         -0.6627,  0.2078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -0.8902, -0.9922,  0.2078,  0.9843, -0.2941, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000,  0.0902,  0.9843,  0.4902, -0.9843, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -0.9137,  0.4902,  0.9843, -0.4510, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.8902,  0.7647,  0.2549,\n","         -0.1529, -0.9922, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3647,  0.8824,  0.9843,\n","          0.9843, -0.0667, -0.8039, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6471,  0.4588,\n","          0.9843,  0.9843,  0.1765, -0.7882, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n","         -0.2706,  0.9765,  0.9843,  0.4667, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000,  0.9529,  0.9843,  0.9529, -0.4980, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6392,  0.0196,\n","          0.4353,  0.9843,  0.9843,  0.6235, -0.9843, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -0.6941,  0.1608,  0.7961,  0.9843,\n","          0.9843,  0.9843,  0.9608,  0.4275, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -0.8118, -0.1059,  0.7333,  0.9843,  0.9843,  0.9843,\n","          0.9843,  0.5765, -0.3882, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -0.8196, -0.4824,  0.6706,  0.9843,  0.9843,  0.9843,  0.9843,  0.5529,\n","         -0.3647, -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,  0.3412,\n","          0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.5294, -0.3725, -0.9294,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.3490,  0.7725,  0.9843,\n","          0.9843,  0.9843,  0.9843,  0.9137,  0.0431, -0.9137, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000,  0.0667,  0.9843,  0.9843,  0.9843,\n","          0.6627,  0.0588,  0.0353, -0.8745, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000],\n","        [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","         -1.0000, -1.0000, -1.0000, -1.0000]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"F9uT5uI2PvVH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1-ycdv4uw28Z","colab_type":"text"},"source":["### 2. 모델 구축(생성자 & 구분자)"]},{"cell_type":"markdown","metadata":{"id":"OR72imPjnd7E","colab_type":"text"},"source":["#### 생성자(Generator) 구축\n","- 랜덤 벡터 ‘z’를 입력으로 받아 가짜 이미지를 출력하는 함수\n","  - z : 단순하게 균등 분포(Uniform Distribution)나 정규 분포(Normal Distribution)에서 무작위로 추출된 값\n","  - z 벡터가 존재하는 공간을 잠재 공간이라 부른다.\n","\n","  <img src=\"https://files.slack.com/files-pri/T25783BPY-F9RFJ3VDJ/picture4.png?pub_secret=da0323f283\" width=\"450px\" height=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"4yMbzyU8uOy9","colab_type":"text"},"source":["- 선형 레이어\n","  - 모든 뉴런이 이전 레이어의 모든 뉴런과 연결되는 가장 단순한 구조"]},{"cell_type":"markdown","metadata":{"id":"mbhu4xFsnd9M","colab_type":"text"},"source":["#### 사용하는 모델\n","- 4개의 선형 레이어\n","  - 100차원 랜덤벡터 => 레이어(256개 뉴런)  \n","    => 레이어(512) => 레이어(1024) => 마지막 레이어(28x28)\n","- 활성 함수로는 LeakyReLU ( 0보다 낮은 값은 0.2(설정값)을 곱 )\n","  - 마지막 레이어 : Tanh ( 픽셀값이 -1~1 이기 때문에 )"]},{"cell_type":"code","metadata":{"id":"sCVnOKzNnd_U","colab_type":"code","colab":{}},"source":["# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n","class Generator(nn.Module):\n","\n","  # 네트워크 구조\n","    def __init__(self):\n","      super(Generator, self).__init__()\n","      self.main = nn.Sequential(\n","        nn.Linear(in_features=100, out_features=256),\n","        nn.LeakyReLU(0.2, inplace=True),\n","        nn.Linear(in_features=256, out_features=512),\n","        nn.LeakyReLU(0.2, inplace=True),\n","        nn.Linear(in_features=512, out_features=1024),\n","        nn.LeakyReLU(0.2, inplace=True),\n","        nn.Linear(in_features=1024, out_features=28*28),\n","        nn.Tanh())\n","    \n","  # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n","  # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n","    def forward(self, inputs):\n","      return self.main(inputs).view(-1, 1, 28, 28)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sfr-U67d9qMH","colab":{}},"source":["# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n","class Generator(nn.Module):\n","\n","  # 네트워크 구조\n","    def __init__(self):\n","      super(Generator, self).__init__()\n","      self.main = nn.Sequential(\n","        nn.Linear(in_features=100, out_features=256),\n","        nn.LeakyReLU(0.2),\n","        nn.Linear(in_features=256, out_features=512),\n","        nn.LeakyReLU(0.2),\n","        nn.Linear(in_features=512, out_features=1024),\n","        nn.LeakyReLU(0.2),\n","        nn.Linear(in_features=1024, out_features=28*28),\n","        nn.Tanh())\n","    \n","  # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n","  # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n","    def forward(self, inputs):\n","      return self.main(inputs).view(-1, 1, 28, 28)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"goYtqQYpv1HS","colab":{}},"source":["# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n","class Generator(nn.Module):\n","\n","  # 네트워크 구조\n","    def __init__(self):\n","      super(Generator, self).__init__()\n","      self.main = nn.Sequential(\n","        nn.Linear(in_features=100, out_features=256),\n","        nn.LeakyReLU(0.2),\n","        nn.Linear(in_features=256, out_features=512),\n","        nn.LeakyReLU(0.2),\n","        nn.Linear(in_features=512, out_features=1024),\n","        nn.LeakyReLU(0.2),\n","        nn.Linear(in_features=1024, out_features=28*28),\n","        nn.Tanh())\n","    \n","  # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n","  # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n","    def forward(self, inputs):\n","      return self.main(inputs).view(-1, 1, 28, 28)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"05X_WGjwvv8y"},"source":["#### 구분자(Discriminator) 구축\n","  - 이미지를 입력으로 받고 그 이미지가 진짜일 확률을 0과 1 사이의 숫자 하나로 출력하는 함수"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a70aRCRav8Pn"},"source":["#### 사용하는 모델\n","- 4개의 선형 레이어\n","  - 이미지(28x28) => 레이어(1024 뉴런)  \n","    => 레이어(512) => 레이어(256) => 마지막 예측 레이어(1)\n","- 활성 함수로는 LeakyReLU ( 0보다 낮은 값은 0.2(설정값)을 곱 )\n","  - 마지막 레이어 : Sigmoid ( 출략값이 0~1 이기 때문에 )\n","- 드롭아웃 층\n","  - 과적합(Overfitting, 오버피팅)되는 것을 방지\n","  - 구분자가 생성자보다 지나치게 빨리 학습되는 것 막기 위함."]},{"cell_type":"code","metadata":{"id":"nun-_C08neDt","colab_type":"code","colab":{}},"source":["# 구분자는 이미지를 입력으로 받아 이미지가 진짜인지 가짜인지 출력한다.\n","class Discriminator(nn.Module):\n","    \n","# 네트워크 구조\n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    self.main = nn.Sequential(\n","      nn.Linear(in_features=28*28, out_features=1024),\n","      nn.LeakyReLU(0.2, inplace=True),\n","      nn.Dropout(inplace=True),\n","      nn.Linear(in_features=1024, out_features=512),\n","      nn.LeakyReLU(0.2, inplace=True),\n","      nn.Dropout(inplace=True),\n","      nn.Linear(in_features=512, out_features=256),\n","      nn.LeakyReLU(0.2, inplace=True),\n","      nn.Dropout(inplace=True),\n","      nn.Linear(in_features=256, out_features=1),\n","      nn.Sigmoid())\n","    \n","  # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아\n","  # 이미지가 진짜일 확률을 0~1 사이로 출력한다.\n","  def forward(self, inputs):\n","    inputs = inputs.view(-1, 28*28)\n","    return self.main(inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wu2cTq3P9ivw","colab":{}},"source":["# 구분자는 이미지를 입력으로 받아 이미지가 진짜인지 가짜인지 출력한다.\n","class Discriminator(nn.Module):\n","    \n","# 네트워크 구조\n","  def __init__(self):\n","    super(Discriminator, self).__init__()\n","    self.main = nn.Sequential(\n","      nn.Linear(in_features=28*28, out_features=1024),\n","      nn.LeakyReLU(0.2, ),\n","      nn.Dropout(),\n","      nn.Linear(in_features=1024, out_features=512),\n","      nn.LeakyReLU(0.2, ),\n","      nn.Dropout(),\n","      nn.Linear(in_features=512, out_features=256),\n","      nn.LeakyReLU(0.2, ),\n","      nn.Dropout(),\n","      nn.Linear(in_features=256, out_features=1),\n","      nn.Sigmoid())\n","    \n","  # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아\n","  # 이미지가 진짜일 확률을 0~1 사이로 출력한다.\n","  def forward(self, inputs):\n","    inputs = inputs.view(-1, 28*28)\n","    return self.main(inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eIM3DZ3hwx3T","colab_type":"text"},"source":["#### 생성자 & 구분자 객체 생성"]},{"cell_type":"code","metadata":{"id":"2d8iOT10neF0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"28f61cb2-dc0c-4169-f283-1fc0ea668193","executionInfo":{"status":"ok","timestamp":1589251511229,"user_tz":-540,"elapsed":631,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}}},"source":["G = Generator()\n","D = Discriminator()\n","\n","# use_gpu=False\n","if use_gpu:\n","    print(\"using_GPU\")\n","    G.cuda()\n","    D.cuda()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["using_GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fMG3fmwRneIE","colab_type":"text"},"source":["### 손실함수 & 최적화기법 지정"]},{"cell_type":"markdown","metadata":{"id":"L2q7MMFLneKF","colab_type":"text"},"source":["- 손실 함수\n","  - Binary Cross Entropy Loss function\n","    - 확률이 정답에 가까워지면 낮아지고, 멀면 높아진다.\n","    - 낮추는 것이 목표\n","- 최적화 기법 \n","  - Adam\n","    - 매개변수마다 업데이트 속도를 최적으로 조절하는 효육적인 최적화 기법"]},{"cell_type":"code","metadata":{"id":"cg0cnKjgneMU","colab_type":"code","colab":{}},"source":["# Binary Cross Entropy loss\n","criterion = nn.BCELoss()\n","\n","# 생성자의 매개 변수를 최적화하는 Adam optimizer\n","G_optimizer = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n","# 구분자의 매개 변수를 최적화하는 Adam optimizer\n","D_optimizer = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9AJKZgBq0Vfe","colab_type":"text"},"source":["### 시각화 함수"]},{"cell_type":"code","metadata":{"id":"Zu7XcfXo0V27","colab_type":"code","colab":{}},"source":["# 학습 결과 시각화하기\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","def square_plot(data, path):\n","    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n","       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n","\n","    if type(data) == list:\n","\t    data = np.concatenate(data)\n","    # normalize data for display\n","    data = (data - data.min()) / (data.max() - data.min())\n","\n","    # force the number of filters to be square\n","    n = int(np.ceil(np.sqrt(data.shape[0])))\n","\n","    padding = (((0, n ** 2 - data.shape[0]) ,\n","                (0, 1), (0, 1))  # add some space between filters\n","               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n","    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n","\n","    # tilethe filters into an image\n","    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n","\n","    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n","\n","    plt.imsave(path, data, cmap='gray')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kQMfpwd0cbQ","colab_type":"code","outputId":"5cf42dcb-9cc9-4966-9e20-11d0253c2c1e","executionInfo":{"status":"ok","timestamp":1589251516970,"user_tz":-540,"elapsed":605,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["if leave_log:\n","    train_hist = {}\n","    train_hist['D_losses'] = []\n","    train_hist['G_losses'] = []\n","    generated_images = []\n","    \n","z_fixed = Variable(torch.randn(5 * 5, 100), volatile=True)\n","if use_gpu:\n","    z_fixed = z_fixed.cuda()"],"execution_count":30,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  import sys\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Hsil_3u1neOJ","colab_type":"text"},"source":["### 모델 반복학습"]},{"cell_type":"markdown","metadata":{"id":"vtMVmtJpneQk","colab_type":"text"},"source":["- 100 epoch  \n","  => 데이터셋을 100번 순회\n","- 60 batch size  \n","  => epoch마다 60개의 데이터를 가져오겠다.\n","\n","MNIST 학습 데이터의 개수가 6만개이니 1에폭마다 1000번씩 학습이 이루어지는 셈이다."]},{"cell_type":"code","metadata":{"id":"zHQiuAKHneSo","colab_type":"code","outputId":"dc00c6f4-3847-43c1-965f-00097ad1a689","executionInfo":{"status":"ok","timestamp":1589253053494,"user_tz":-540,"elapsed":1532938,"user":{"displayName":"ihwan","photoUrl":"","userId":"01231233551275977316"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from time import time\n","\n","start = time()\n","# 데이터셋을 100번 돌며 학습한다.\n","for epoch in range(100):\n","    \n","    if leave_log:\n","        D_losses = []\n","        G_losses = []\n","    \n","    # 한번에 batch_size만큼 데이터를 가져온다.\n","    for real_data, _ in dataloader:\n","        batch_size = real_data.size(0)\n","        \n","        # 데이터를 pytorch의 변수로 변환한다.\n","        real_data = Variable(real_data)\n","\n","        ### 구분자 학습시키기\n","\n","        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n","        # 정답지에 해당하는 변수를 만든다.\n","        target_real = Variable(torch.ones(batch_size, 1))\n","        target_fake = Variable(torch.zeros(batch_size, 1))\n","         \n","        if use_gpu:\n","            real_data, target_real, target_fake = real_data.cuda(), target_real.cuda(), target_fake.cuda()\n","            \n","        # 진짜 이미지를 구분자에 넣는다.\n","        D_result_from_real = D(real_data)\n","        # 구분자의 출력값이 정답지인 1에서 멀수록 loss가 높아진다.\n","        D_loss_real = criterion(D_result_from_real, target_real)\n","\n","        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n","        z = Variable(torch.randn((batch_size, 100)))\n","        \n","        if use_gpu:\n","            z = z.cuda()\n","            \n","        # 생성자로 가짜 이미지를 생성한다.\n","        fake_data = G(z)\n","        \n","        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n","        D_result_from_fake = D(fake_data)\n","        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n","        D_loss_fake = criterion(D_result_from_fake, target_fake)\n","        \n","        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n","        D_loss = D_loss_real + D_loss_fake\n","        \n","        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n","        D.zero_grad()\n","        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n","        D_loss.backward()\n","        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n","        D_optimizer.step()\n","        \n","        if leave_log:\n","            # D_losses.append(D_loss.data[0])\n","            D_losses.append(D_loss.item())\n","\n","        # train generator G\n","\n","        ### 생성자 학습시키기\n","        \n","        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n","        z = Variable(torch.randn((batch_size, 100)))\n","        \n","        if use_gpu:\n","            z = z.cuda()\n","        \n","        # 생성자로 가짜 이미지를 생성한다.\n","        fake_data = G(z)\n","        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n","        D_result_from_fake = D(fake_data)\n","        # 생성자의 입장에서 구분자의 출력값이 1에서 멀수록 loss가 높아진다.\n","        G_loss = criterion(D_result_from_fake, target_real)\n","        \n","        # 생성자의 매개 변수의 미분값을 0으로 초기화한다.\n","        G.zero_grad()\n","        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n","        G_loss.backward()\n","        # 최적화 기법을 이용해 생성자의 매개 변수를 업데이트한다.\n","        G_optimizer.step()\n","        \n","        if leave_log:\n","            # G_losses.append(G_loss.data[0])\n","            G_losses.append(G_loss.item())\n","    if leave_log:\n","        # true_positive_rate = (D_result_from_real > 0.5).float().mean().data[0]\n","        # true_negative_rate = (D_result_from_fake < 0.5).float().mean().data[0]\n","        true_positive_rate = (D_result_from_real > 0.5).float().mean().item()\n","        true_negative_rate = (D_result_from_fake < 0.5).float().mean().item()\n","        base_message = (\"Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} \"\n","                        \"True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}\"\n","                       )\n","        message = base_message.format(\n","                    epoch=epoch,\n","                    d_loss=sum(D_losses)/len(D_losses),\n","                    g_loss=sum(G_losses)/len(G_losses),\n","                    tpr=true_positive_rate,\n","                    tnr=true_negative_rate\n","        )\n","        print(message)\n","    \n","    if leave_log:\n","        fake_data_fixed = G(z_fixed)\n","        image_path = result_dir + '/epoch{}.png'.format(epoch)\n","        square_plot(fake_data_fixed.view(25, 28, 28).cpu().data.numpy(), path=image_path)\n","        generated_images.append(image_path)\n","    \n","    if leave_log:\n","        train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n","        train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n","\n","torch.save(G.state_dict(), \"gan_generator.pkl\")\n","torch.save(D.state_dict(), \"gan_discriminator.pkl\")\n","with open('gan_train_history.pkl', 'wb') as f:\n","    pickle.dump(train_hist, f)\n","\n","generated_image_array = [imageio.imread(generated_image) for generated_image in generated_images]\n","imageio.mimsave(result_dir + '/GAN_generation.gif', generated_image_array, fps=5)\n","\n","print(\"end_time : \", str(start-time()))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Epoch: 0   D Loss: 0.834779 G Loss: 2.08193  True Positive Rate: 96.7% True Negative Rate: 100.0%\n","Epoch: 1   D Loss: 0.657114 G Loss: 2.56907  True Positive Rate: 81.7% True Negative Rate: 100.0%\n","Epoch: 2   D Loss: 0.612643 G Loss: 2.54937  True Positive Rate: 86.7% True Negative Rate: 93.3%\n","Epoch: 3   D Loss: 0.706754 G Loss: 2.13505  True Positive Rate: 86.7% True Negative Rate: 93.3%\n","Epoch: 4   D Loss: 0.75937  G Loss: 1.92004  True Positive Rate: 83.3% True Negative Rate: 95.0%\n","Epoch: 5   D Loss: 0.882905 G Loss: 1.61766  True Positive Rate: 81.7% True Negative Rate: 90.0%\n","Epoch: 6   D Loss: 0.987596 G Loss: 1.37331  True Positive Rate: 90.0% True Negative Rate: 93.3%\n","Epoch: 7   D Loss: 1.01964  G Loss: 1.29193  True Positive Rate: 63.3% True Negative Rate: 73.3%\n","Epoch: 8   D Loss: 1.08434  G Loss: 1.1851   True Positive Rate: 63.3% True Negative Rate: 96.7%\n","Epoch: 9   D Loss: 1.10501  G Loss: 1.14809  True Positive Rate: 31.7% True Negative Rate: 81.7%\n","Epoch: 10  D Loss: 1.15249  G Loss: 1.06628  True Positive Rate: 55.0% True Negative Rate: 85.0%\n","Epoch: 11  D Loss: 1.17917  G Loss: 1.01886  True Positive Rate: 53.3% True Negative Rate: 66.7%\n","Epoch: 12  D Loss: 1.19642  G Loss: 0.992194 True Positive Rate: 56.7% True Negative Rate: 91.7%\n","Epoch: 13  D Loss: 1.20276  G Loss: 0.982558 True Positive Rate: 78.3% True Negative Rate: 78.3%\n","Epoch: 14  D Loss: 1.20696  G Loss: 0.97716  True Positive Rate: 63.3% True Negative Rate: 75.0%\n","Epoch: 15  D Loss: 1.21658  G Loss: 0.964357 True Positive Rate: 61.7% True Negative Rate: 73.3%\n","Epoch: 16  D Loss: 1.21758  G Loss: 0.962119 True Positive Rate: 63.3% True Negative Rate: 73.3%\n","Epoch: 17  D Loss: 1.22963  G Loss: 0.942173 True Positive Rate: 60.0% True Negative Rate: 60.0%\n","Epoch: 18  D Loss: 1.23699  G Loss: 0.930146 True Positive Rate: 60.0% True Negative Rate: 76.7%\n","Epoch: 19  D Loss: 1.234    G Loss: 0.929013 True Positive Rate: 60.0% True Negative Rate: 91.7%\n","Epoch: 20  D Loss: 1.24315  G Loss: 0.920455 True Positive Rate: 48.3% True Negative Rate: 76.7%\n","Epoch: 21  D Loss: 1.24902  G Loss: 0.912335 True Positive Rate: 51.7% True Negative Rate: 80.0%\n","Epoch: 22  D Loss: 1.25338  G Loss: 0.905847 True Positive Rate: 48.3% True Negative Rate: 68.3%\n","Epoch: 23  D Loss: 1.25254  G Loss: 0.911363 True Positive Rate: 46.7% True Negative Rate: 61.7%\n","Epoch: 24  D Loss: 1.24808  G Loss: 0.914157 True Positive Rate: 61.7% True Negative Rate: 71.7%\n","Epoch: 25  D Loss: 1.25304  G Loss: 0.911429 True Positive Rate: 58.3% True Negative Rate: 73.3%\n","Epoch: 26  D Loss: 1.25212  G Loss: 0.904126 True Positive Rate: 71.7% True Negative Rate: 65.0%\n","Epoch: 27  D Loss: 1.25295  G Loss: 0.905883 True Positive Rate: 48.3% True Negative Rate: 66.7%\n","Epoch: 28  D Loss: 1.2638   G Loss: 0.890625 True Positive Rate: 56.7% True Negative Rate: 70.0%\n","Epoch: 29  D Loss: 1.2506   G Loss: 0.914604 True Positive Rate: 60.0% True Negative Rate: 70.0%\n","Epoch: 30  D Loss: 1.2524   G Loss: 0.909631 True Positive Rate: 40.0% True Negative Rate: 73.3%\n","Epoch: 31  D Loss: 1.25324  G Loss: 0.905525 True Positive Rate: 70.0% True Negative Rate: 70.0%\n","Epoch: 32  D Loss: 1.2528   G Loss: 0.907883 True Positive Rate: 68.3% True Negative Rate: 75.0%\n","Epoch: 33  D Loss: 1.25407  G Loss: 0.904225 True Positive Rate: 48.3% True Negative Rate: 71.7%\n","Epoch: 34  D Loss: 1.2573   G Loss: 0.902739 True Positive Rate: 56.7% True Negative Rate: 75.0%\n","Epoch: 35  D Loss: 1.2554   G Loss: 0.90378  True Positive Rate: 63.3% True Negative Rate: 66.7%\n","Epoch: 36  D Loss: 1.25695  G Loss: 0.900585 True Positive Rate: 65.0% True Negative Rate: 75.0%\n","Epoch: 37  D Loss: 1.25925  G Loss: 0.900277 True Positive Rate: 66.7% True Negative Rate: 73.3%\n","Epoch: 38  D Loss: 1.25574  G Loss: 0.906209 True Positive Rate: 45.0% True Negative Rate: 75.0%\n","Epoch: 39  D Loss: 1.25718  G Loss: 0.90459  True Positive Rate: 63.3% True Negative Rate: 61.7%\n","Epoch: 40  D Loss: 1.25608  G Loss: 0.904761 True Positive Rate: 46.7% True Negative Rate: 76.7%\n","Epoch: 41  D Loss: 1.25958  G Loss: 0.900327 True Positive Rate: 65.0% True Negative Rate: 60.0%\n","Epoch: 42  D Loss: 1.25487  G Loss: 0.902313 True Positive Rate: 63.3% True Negative Rate: 61.7%\n","Epoch: 43  D Loss: 1.25998  G Loss: 0.898997 True Positive Rate: 56.7% True Negative Rate: 71.7%\n","Epoch: 44  D Loss: 1.26172  G Loss: 0.898563 True Positive Rate: 60.0% True Negative Rate: 61.7%\n","Epoch: 45  D Loss: 1.26224  G Loss: 0.895672 True Positive Rate: 56.7% True Negative Rate: 85.0%\n","Epoch: 46  D Loss: 1.26003  G Loss: 0.893217 True Positive Rate: 50.0% True Negative Rate: 78.3%\n","Epoch: 47  D Loss: 1.25872  G Loss: 0.90065  True Positive Rate: 66.7% True Negative Rate: 61.7%\n","Epoch: 48  D Loss: 1.26257  G Loss: 0.894229 True Positive Rate: 45.0% True Negative Rate: 73.3%\n","Epoch: 49  D Loss: 1.26748  G Loss: 0.884532 True Positive Rate: 51.7% True Negative Rate: 76.7%\n","Epoch: 50  D Loss: 1.26582  G Loss: 0.890615 True Positive Rate: 68.3% True Negative Rate: 76.7%\n","Epoch: 51  D Loss: 1.2618   G Loss: 0.897479 True Positive Rate: 60.0% True Negative Rate: 76.7%\n","Epoch: 52  D Loss: 1.26749  G Loss: 0.886878 True Positive Rate: 55.0% True Negative Rate: 56.7%\n","Epoch: 53  D Loss: 1.25812  G Loss: 0.898846 True Positive Rate: 48.3% True Negative Rate: 73.3%\n","Epoch: 54  D Loss: 1.26506  G Loss: 0.893598 True Positive Rate: 53.3% True Negative Rate: 53.3%\n","Epoch: 55  D Loss: 1.26288  G Loss: 0.894837 True Positive Rate: 46.7% True Negative Rate: 68.3%\n","Epoch: 56  D Loss: 1.26273  G Loss: 0.892667 True Positive Rate: 38.3% True Negative Rate: 66.7%\n","Epoch: 57  D Loss: 1.26597  G Loss: 0.891773 True Positive Rate: 61.7% True Negative Rate: 71.7%\n","Epoch: 58  D Loss: 1.26587  G Loss: 0.883927 True Positive Rate: 70.0% True Negative Rate: 65.0%\n","Epoch: 59  D Loss: 1.26799  G Loss: 0.882514 True Positive Rate: 60.0% True Negative Rate: 76.7%\n","Epoch: 60  D Loss: 1.26774  G Loss: 0.8838   True Positive Rate: 63.3% True Negative Rate: 51.7%\n","Epoch: 61  D Loss: 1.2675   G Loss: 0.884022 True Positive Rate: 66.7% True Negative Rate: 61.7%\n","Epoch: 62  D Loss: 1.27164  G Loss: 0.881134 True Positive Rate: 58.3% True Negative Rate: 63.3%\n","Epoch: 63  D Loss: 1.27111  G Loss: 0.8785   True Positive Rate: 56.7% True Negative Rate: 71.7%\n","Epoch: 64  D Loss: 1.2746   G Loss: 0.873371 True Positive Rate: 51.7% True Negative Rate: 63.3%\n","Epoch: 65  D Loss: 1.27312  G Loss: 0.876392 True Positive Rate: 51.7% True Negative Rate: 75.0%\n","Epoch: 66  D Loss: 1.27211  G Loss: 0.876632 True Positive Rate: 61.7% True Negative Rate: 73.3%\n","Epoch: 67  D Loss: 1.27283  G Loss: 0.879871 True Positive Rate: 53.3% True Negative Rate: 65.0%\n","Epoch: 68  D Loss: 1.27482  G Loss: 0.873932 True Positive Rate: 55.0% True Negative Rate: 66.7%\n","Epoch: 69  D Loss: 1.27878  G Loss: 0.867731 True Positive Rate: 65.0% True Negative Rate: 66.7%\n","Epoch: 70  D Loss: 1.27776  G Loss: 0.871957 True Positive Rate: 48.3% True Negative Rate: 63.3%\n","Epoch: 71  D Loss: 1.27653  G Loss: 0.871397 True Positive Rate: 51.7% True Negative Rate: 66.7%\n","Epoch: 72  D Loss: 1.27551  G Loss: 0.874449 True Positive Rate: 50.0% True Negative Rate: 75.0%\n","Epoch: 73  D Loss: 1.27718  G Loss: 0.872375 True Positive Rate: 48.3% True Negative Rate: 65.0%\n","Epoch: 74  D Loss: 1.27461  G Loss: 0.874077 True Positive Rate: 65.0% True Negative Rate: 71.7%\n","Epoch: 75  D Loss: 1.27667  G Loss: 0.869086 True Positive Rate: 60.0% True Negative Rate: 73.3%\n","Epoch: 76  D Loss: 1.27228  G Loss: 0.878587 True Positive Rate: 43.3% True Negative Rate: 55.0%\n","Epoch: 77  D Loss: 1.27591  G Loss: 0.872235 True Positive Rate: 60.0% True Negative Rate: 83.3%\n","Epoch: 78  D Loss: 1.27649  G Loss: 0.874809 True Positive Rate: 56.7% True Negative Rate: 71.7%\n","Epoch: 79  D Loss: 1.27659  G Loss: 0.868819 True Positive Rate: 53.3% True Negative Rate: 83.3%\n","Epoch: 80  D Loss: 1.27781  G Loss: 0.869608 True Positive Rate: 60.0% True Negative Rate: 60.0%\n","Epoch: 81  D Loss: 1.27866  G Loss: 0.8654   True Positive Rate: 41.7% True Negative Rate: 80.0%\n","Epoch: 82  D Loss: 1.27704  G Loss: 0.871966 True Positive Rate: 58.3% True Negative Rate: 66.7%\n","Epoch: 83  D Loss: 1.27551  G Loss: 0.871503 True Positive Rate: 61.7% True Negative Rate: 63.3%\n","Epoch: 84  D Loss: 1.28037  G Loss: 0.865706 True Positive Rate: 55.0% True Negative Rate: 66.7%\n","Epoch: 85  D Loss: 1.28211  G Loss: 0.862888 True Positive Rate: 51.7% True Negative Rate: 73.3%\n","Epoch: 86  D Loss: 1.27636  G Loss: 0.866843 True Positive Rate: 53.3% True Negative Rate: 75.0%\n","Epoch: 87  D Loss: 1.2793   G Loss: 0.869928 True Positive Rate: 68.3% True Negative Rate: 48.3%\n","Epoch: 88  D Loss: 1.27888  G Loss: 0.867298 True Positive Rate: 65.0% True Negative Rate: 65.0%\n","Epoch: 89  D Loss: 1.27933  G Loss: 0.86836  True Positive Rate: 56.7% True Negative Rate: 73.3%\n","Epoch: 90  D Loss: 1.28225  G Loss: 0.862925 True Positive Rate: 61.7% True Negative Rate: 66.7%\n","Epoch: 91  D Loss: 1.28005  G Loss: 0.861402 True Positive Rate: 50.0% True Negative Rate: 75.0%\n","Epoch: 92  D Loss: 1.28404  G Loss: 0.860688 True Positive Rate: 51.7% True Negative Rate: 73.3%\n","Epoch: 93  D Loss: 1.28368  G Loss: 0.860744 True Positive Rate: 58.3% True Negative Rate: 51.7%\n","Epoch: 94  D Loss: 1.28499  G Loss: 0.862889 True Positive Rate: 85.0% True Negative Rate: 56.7%\n","Epoch: 95  D Loss: 1.28214  G Loss: 0.865251 True Positive Rate: 50.0% True Negative Rate: 66.7%\n","Epoch: 96  D Loss: 1.28675  G Loss: 0.855099 True Positive Rate: 65.0% True Negative Rate: 60.0%\n","Epoch: 97  D Loss: 1.28469  G Loss: 0.858611 True Positive Rate: 61.7% True Negative Rate: 68.3%\n","Epoch: 98  D Loss: 1.28523  G Loss: 0.855941 True Positive Rate: 63.3% True Negative Rate: 68.3%\n","Epoch: 99  D Loss: 1.28384  G Loss: 0.86133  True Positive Rate: 66.7% True Negative Rate: 63.3%\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["end_time :  -1532.1076147556305\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x-jJAg-iiqre","colab_type":"text"},"source":["<hr>\n","\n","# first run\n","- Loss의 변화가 거의 없다 시피하다..\n","\n","~~~\n","Epoch: 0   D Loss: 1.28488  G Loss: 0.859424 True Positive Rate: 55.0% True Negative Rate: 73.3%\n","Epoch: 1   D Loss: 1.28413  G Loss: 0.858598 True Positive Rate: 61.7% True Negative Rate: 66.7%\n","Epoch: 2   D Loss: 1.28431  G Loss: 0.858152 True Positive Rate: 66.7% True Negative Rate: 73.3%\n","Epoch: 3   D Loss: 1.28527  G Loss: 0.858293 True Positive Rate: 61.7% True Negative Rate: 63.3%\n","Epoch: 4   D Loss: 1.28188  G Loss: 0.861645 True Positive Rate: 68.3% True Negative Rate: 48.3%\n","Epoch: 5   D Loss: 1.28678  G Loss: 0.856552 True Positive Rate: 66.7% True Negative Rate: 80.0%\n","Epoch: 6   D Loss: 1.28599  G Loss: 0.856698 True Positive Rate: 55.0% True Negative Rate: 61.7%\n","Epoch: 7   D Loss: 1.2889   G Loss: 0.852717 True Positive Rate: 58.3% True Negative Rate: 75.0%\n","Epoch: 8   D Loss: 1.28584  G Loss: 0.85264  True Positive Rate: 53.3% True Negative Rate: 81.7%\n","Epoch: 9   D Loss: 1.28204  G Loss: 0.868227 True Positive Rate: 60.0% True Negative Rate: 61.7%\n","Epoch: 10  D Loss: 1.28223  G Loss: 0.862518 True Positive Rate: 45.0% True Negative Rate: 70.0%\n","Epoch: 11  D Loss: 1.28387  G Loss: 0.858618 True Positive Rate: 60.0% True Negative Rate: 48.3%\n","Epoch: 12  D Loss: 1.28693  G Loss: 0.854806 True Positive Rate: 65.0% True Negative Rate: 63.3%\n","Epoch: 13  D Loss: 1.28463  G Loss: 0.864841 True Positive Rate: 56.7% True Negative Rate: 58.3%\n","Epoch: 14  D Loss: 1.28875  G Loss: 0.854147 True Positive Rate: 50.0% True Negative Rate: 73.3%\n","Epoch: 15  D Loss: 1.28705  G Loss: 0.852105 True Positive Rate: 65.0% True Negative Rate: 56.7%\n","Epoch: 16  D Loss: 1.28775  G Loss: 0.853729 True Positive Rate: 53.3% True Negative Rate: 65.0%\n","Epoch: 17  D Loss: 1.2865   G Loss: 0.855204 True Positive Rate: 50.0% True Negative Rate: 53.3%\n","Epoch: 18  D Loss: 1.28987  G Loss: 0.848932 True Positive Rate: 55.0% True Negative Rate: 66.7%\n","Epoch: 19  D Loss: 1.2877   G Loss: 0.856839 True Positive Rate: 61.7% True Negative Rate: 78.3%\n","Epoch: 20  D Loss: 1.28806  G Loss: 0.853432 True Positive Rate: 51.7% True Negative Rate: 66.7%\n","Epoch: 21  D Loss: 1.28827  G Loss: 0.852255 True Positive Rate: 65.0% True Negative Rate: 73.3%\n","Epoch: 22  D Loss: 1.29219  G Loss: 0.851609 True Positive Rate: 65.0% True Negative Rate: 66.7%\n","Epoch: 23  D Loss: 1.29008  G Loss: 0.853978 True Positive Rate: 61.7% True Negative Rate: 55.0%\n","Epoch: 24  D Loss: 1.28422  G Loss: 0.858663 True Positive Rate: 68.3% True Negative Rate: 66.7%\n","Epoch: 25  D Loss: 1.28867  G Loss: 0.856496 True Positive Rate: 38.3% True Negative Rate: 75.0%\n","Epoch: 26  D Loss: 1.2885   G Loss: 0.852642 True Positive Rate: 66.7% True Negative Rate: 71.7%\n","Epoch: 27  D Loss: 1.29227  G Loss: 0.846885 True Positive Rate: 63.3% True Negative Rate: 70.0%\n","Epoch: 28  D Loss: 1.28761  G Loss: 0.857693 True Positive Rate: 50.0% True Negative Rate: 65.0%\n","Epoch: 29  D Loss: 1.29003  G Loss: 0.846687 True Positive Rate: 61.7% True Negative Rate: 61.7%\n","Epoch: 30  D Loss: 1.29096  G Loss: 0.852305 True Positive Rate: 65.0% True Negative Rate: 65.0%\n","Epoch: 31  D Loss: 1.29236  G Loss: 0.851856 True Positive Rate: 55.0% True Negative Rate: 78.3%\n","Epoch: 32  D Loss: 1.29017  G Loss: 0.853993 True Positive Rate: 66.7% True Negative Rate: 75.0%\n","Epoch: 33  D Loss: 1.29198  G Loss: 0.849653 True Positive Rate: 56.7% True Negative Rate: 66.7%\n","Epoch: 34  D Loss: 1.29369  G Loss: 0.842581 True Positive Rate: 40.0% True Negative Rate: 56.7%\n","Epoch: 35  D Loss: 1.29274  G Loss: 0.846047 True Positive Rate: 58.3% True Negative Rate: 66.7%\n","Epoch: 36  D Loss: 1.28996  G Loss: 0.85489  True Positive Rate: 60.0% True Negative Rate: 73.3%\n","Epoch: 37  D Loss: 1.29057  G Loss: 0.849331 True Positive Rate: 56.7% True Negative Rate: 68.3%\n","Epoch: 38  D Loss: 1.29054  G Loss: 0.847588 True Positive Rate: 46.7% True Negative Rate: 56.7%\n","Epoch: 39  D Loss: 1.29042  G Loss: 0.850941 True Positive Rate: 53.3% True Negative Rate: 63.3%\n","Epoch: 40  D Loss: 1.29425  G Loss: 0.84697  True Positive Rate: 60.0% True Negative Rate: 60.0%\n","Epoch: 41  D Loss: 1.28904  G Loss: 0.853235 True Positive Rate: 56.7% True Negative Rate: 71.7%\n","Epoch: 42  D Loss: 1.28986  G Loss: 0.848996 True Positive Rate: 65.0% True Negative Rate: 63.3%\n","Epoch: 43  D Loss: 1.29054  G Loss: 0.84903  True Positive Rate: 51.7% True Negative Rate: 65.0%\n","Epoch: 44  D Loss: 1.29249  G Loss: 0.851152 True Positive Rate: 45.0% True Negative Rate: 61.7%\n","Epoch: 45  D Loss: 1.28995  G Loss: 0.850684 True Positive Rate: 50.0% True Negative Rate: 66.7%\n","Epoch: 46  D Loss: 1.28776  G Loss: 0.853122 True Positive Rate: 58.3% True Negative Rate: 51.7%\n","Epoch: 47  D Loss: 1.29712  G Loss: 0.840202 True Positive Rate: 70.0% True Negative Rate: 70.0%\n","Epoch: 48  D Loss: 1.29242  G Loss: 0.847086 True Positive Rate: 58.3% True Negative Rate: 71.7%\n","Epoch: 49  D Loss: 1.29554  G Loss: 0.842557 True Positive Rate: 63.3% True Negative Rate: 70.0%\n","Epoch: 50  D Loss: 1.28994  G Loss: 0.850531 True Positive Rate: 50.0% True Negative Rate: 83.3%\n","Epoch: 52  D Loss: 1.29003  G Loss: 0.852058 True Positive Rate: 70.0% True Negative Rate: 66.7%\n","Epoch: 53  D Loss: 1.29533  G Loss: 0.843466 True Positive Rate: 60.0% True Negative Rate: 73.3%\n","Epoch: 54  D Loss: 1.2958   G Loss: 0.842267 True Positive Rate: 66.7% True Negative Rate: 65.0%\n","Epoch: 55  D Loss: 1.29475  G Loss: 0.842555 True Positive Rate: 51.7% True Negative Rate: 70.0%\n","Epoch: 56  D Loss: 1.29129  G Loss: 0.849518 True Positive Rate: 60.0% True Negative Rate: 70.0%\n","Epoch: 57  D Loss: 1.29485  G Loss: 0.847201 True Positive Rate: 48.3% True Negative Rate: 73.3%\n","Epoch: 58  D Loss: 1.29413  G Loss: 0.850201 True Positive Rate: 63.3% True Negative Rate: 70.0%\n","Epoch: 59  D Loss: 1.29145  G Loss: 0.851198 True Positive Rate: 63.3% True Negative Rate: 71.7%\n","Epoch: 60  D Loss: 1.29564  G Loss: 0.844253 True Positive Rate: 53.3% True Negative Rate: 71.7%\n","Epoch: 61  D Loss: 1.29594  G Loss: 0.843686 True Positive Rate: 50.0% True Negative Rate: 56.7%\n","Epoch: 62  D Loss: 1.29773  G Loss: 0.842452 True Positive Rate: 53.3% True Negative Rate: 58.3%\n","Epoch: 63  D Loss: 1.29497  G Loss: 0.845855 True Positive Rate: 55.0% True Negative Rate: 61.7%\n","Epoch: 64  D Loss: 1.29622  G Loss: 0.842898 True Positive Rate: 55.0% True Negative Rate: 81.7%\n","Epoch: 65  D Loss: 1.29531  G Loss: 0.843889 True Positive Rate: 36.7% True Negative Rate: 61.7%\n","Epoch: 66  D Loss: 1.29516  G Loss: 0.844741 True Positive Rate: 50.0% True Negative Rate: 45.0%\n","Epoch: 67  D Loss: 1.29463  G Loss: 0.845934 True Positive Rate: 63.3% True Negative Rate: 66.7%\n","Epoch: 68  D Loss: 1.29364  G Loss: 0.846999 True Positive Rate: 56.7% True Negative Rate: 58.3%\n","Epoch: 69  D Loss: 1.29156  G Loss: 0.850331 True Positive Rate: 45.0% True Negative Rate: 63.3%\n","Epoch: 70  D Loss: 1.29029  G Loss: 0.849603 True Positive Rate: 65.0% True Negative Rate: 65.0%\n","Epoch: 71  D Loss: 1.29617  G Loss: 0.841776 True Positive Rate: 65.0% True Negative Rate: 65.0%\n","Epoch: 72  D Loss: 1.29618  G Loss: 0.841495 True Positive Rate: 56.7% True Negative Rate: 51.7%\n","Epoch: 73  D Loss: 1.29573  G Loss: 0.841722 True Positive Rate: 46.7% True Negative Rate: 71.7%\n","Epoch: 74  D Loss: 1.29578  G Loss: 0.844592 True Positive Rate: 71.7% True Negative Rate: 65.0%\n","Epoch: 75  D Loss: 1.29558  G Loss: 0.841717 True Positive Rate: 65.0% True Negative Rate: 63.3%\n","Epoch: 76  D Loss: 1.2992   G Loss: 0.839945 True Positive Rate: 45.0% True Negative Rate: 81.7%\n","Epoch: 77  D Loss: 1.29674  G Loss: 0.842683 True Positive Rate: 38.3% True Negative Rate: 63.3%\n","Epoch: 78  D Loss: 1.29457  G Loss: 0.844406 True Positive Rate: 51.7% True Negative Rate: 71.7%\n","Epoch: 79  D Loss: 1.29384  G Loss: 0.84723  True Positive Rate: 78.3% True Negative Rate: 60.0%\n","Epoch: 80  D Loss: 1.29754  G Loss: 0.840516 True Positive Rate: 43.3% True Negative Rate: 66.7%\n","Epoch: 81  D Loss: 1.29538  G Loss: 0.843333 True Positive Rate: 45.0% True Negative Rate: 58.3%\n","Epoch: 82  D Loss: 1.29656  G Loss: 0.841038 True Positive Rate: 63.3% True Negative Rate: 66.7%\n","Epoch: 83  D Loss: 1.29666  G Loss: 0.837022 True Positive Rate: 56.7% True Negative Rate: 80.0%\n","Epoch: 84  D Loss: 1.29536  G Loss: 0.846857 True Positive Rate: 46.7% True Negative Rate: 61.7%\n","Epoch: 85  D Loss: 1.29781  G Loss: 0.840344 True Positive Rate: 58.3% True Negative Rate: 56.7%\n","Epoch: 86  D Loss: 1.2941   G Loss: 0.850732 True Positive Rate: 46.7% True Negative Rate: 75.0%\n","Epoch: 87  D Loss: 1.29563  G Loss: 0.843402 True Positive Rate: 66.7% True Negative Rate: 68.3%\n","Epoch: 88  D Loss: 1.29804  G Loss: 0.845363 True Positive Rate: 50.0% True Negative Rate: 63.3%\n","Epoch: 89  D Loss: 1.29695  G Loss: 0.841782 True Positive Rate: 60.0% True Negative Rate: 71.7%\n","Epoch: 90  D Loss: 1.29569  G Loss: 0.843132 True Positive Rate: 63.3% True Negative Rate: 71.7%\n","Epoch: 91  D Loss: 1.29813  G Loss: 0.837302 True Positive Rate: 60.0% True Negative Rate: 68.3%\n","Epoch: 92  D Loss: 1.29727  G Loss: 0.838173 True Positive Rate: 63.3% True Negative Rate: 60.0%\n","Epoch: 93  D Loss: 1.29404  G Loss: 0.845464 True Positive Rate: 66.7% True Negative Rate: 80.0%\n","Epoch: 94  D Loss: 1.29605  G Loss: 0.844502 True Positive Rate: 56.7% True Negative Rate: 71.7%\n","Epoch: 95  D Loss: 1.29922  G Loss: 0.838849 True Positive Rate: 65.0% True Negative Rate: 73.3%\n","Epoch: 96  D Loss: 1.29795  G Loss: 0.842022 True Positive Rate: 45.0% True Negative Rate: 61.7%\n","Epoch: 97  D Loss: 1.29823  G Loss: 0.835398 True Positive Rate: 65.0% True Negative Rate: 61.7%\n","Epoch: 98  D Loss: 1.29526  G Loss: 0.845145 True Positive Rate: 58.3% True Negative Rate: 53.3%\n","Epoch: 99  D Loss: 1.29671  G Loss: 0.843753 True Positive Rate: 60.0% True Negative Rate: 75.0%\n","/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n","end_time :  -1499.3681178092957\n","\n","~~~"]},{"cell_type":"markdown","metadata":{"id":"CNpgVBr30bnL","colab_type":"text"},"source":["<hr>\n","\n","#  second run\n","앞선 결과와 비슷하다.  \n","랜덤하긴 하지만 결국 비슷하다.\n","\n","~~~\n","Epoch: 0   D Loss: 1.29263  G Loss: 0.845172 True Positive Rate: 46.7% True Negative Rate: 60.0%\n","Epoch: 1   D Loss: 1.29633  G Loss: 0.845791 True Positive Rate: 68.3% True Negative Rate: 63.3%\n","Epoch: 2   D Loss: 1.29359  G Loss: 0.849673 True Positive Rate: 51.7% True Negative Rate: 58.3%\n","Epoch: 3   D Loss: 1.29561  G Loss: 0.842294 True Positive Rate: 60.0% True Negative Rate: 71.7%\n","Epoch: 4   D Loss: 1.29549  G Loss: 0.847163 True Positive Rate: 48.3% True Negative Rate: 70.0%\n","Epoch: 5   D Loss: 1.29396  G Loss: 0.84448  True Positive Rate: 48.3% True Negative Rate: 76.7%\n","Epoch: 6   D Loss: 1.29583  G Loss: 0.843481 True Positive Rate: 55.0% True Negative Rate: 75.0%\n","Epoch: 7   D Loss: 1.29817  G Loss: 0.841995 True Positive Rate: 61.7% True Negative Rate: 60.0%\n","Epoch: 8   D Loss: 1.29501  G Loss: 0.843043 True Positive Rate: 58.3% True Negative Rate: 65.0%\n","Epoch: 9   D Loss: 1.29403  G Loss: 0.844912 True Positive Rate: 48.3% True Negative Rate: 70.0%\n","Epoch: 10  D Loss: 1.29193  G Loss: 0.847046 True Positive Rate: 46.7% True Negative Rate: 71.7%\n","Epoch: 11  D Loss: 1.29358  G Loss: 0.845821 True Positive Rate: 51.7% True Negative Rate: 71.7%\n","Epoch: 12  D Loss: 1.29401  G Loss: 0.845127 True Positive Rate: 56.7% True Negative Rate: 75.0%\n","Epoch: 13  D Loss: 1.29719  G Loss: 0.842042 True Positive Rate: 60.0% True Negative Rate: 46.7%\n","Epoch: 14  D Loss: 1.29604  G Loss: 0.843616 True Positive Rate: 65.0% True Negative Rate: 60.0%\n","Epoch: 15  D Loss: 1.29643  G Loss: 0.843412 True Positive Rate: 51.7% True Negative Rate: 60.0%\n","Epoch: 16  D Loss: 1.29209  G Loss: 0.846979 True Positive Rate: 48.3% True Negative Rate: 73.3%\n","Epoch: 17  D Loss: 1.29206  G Loss: 0.847234 True Positive Rate: 46.7% True Negative Rate: 60.0%\n","Epoch: 18  D Loss: 1.29061  G Loss: 0.851166 True Positive Rate: 61.7% True Negative Rate: 63.3%\n","Epoch: 19  D Loss: 1.29597  G Loss: 0.845586 True Positive Rate: 58.3% True Negative Rate: 63.3%\n","Epoch: 20  D Loss: 1.29613  G Loss: 0.843349 True Positive Rate: 56.7% True Negative Rate: 73.3%\n","Epoch: 21  D Loss: 1.29454  G Loss: 0.8431   True Positive Rate: 63.3% True Negative Rate: 63.3%\n","Epoch: 22  D Loss: 1.28772  G Loss: 0.853526 True Positive Rate: 55.0% True Negative Rate: 65.0%\n","Epoch: 23  D Loss: 1.29323  G Loss: 0.849733 True Positive Rate: 46.7% True Negative Rate: 75.0%\n","Epoch: 24  D Loss: 1.29306  G Loss: 0.849541 True Positive Rate: 68.3% True Negative Rate: 58.3%\n","Epoch: 25  D Loss: 1.29256  G Loss: 0.850123 True Positive Rate: 61.7% True Negative Rate: 71.7%\n","Epoch: 26  D Loss: 1.29409  G Loss: 0.846212 True Positive Rate: 56.7% True Negative Rate: 90.0%\n","Epoch: 27  D Loss: 1.295    G Loss: 0.843389 True Positive Rate: 73.3% True Negative Rate: 65.0%\n","Epoch: 28  D Loss: 1.29488  G Loss: 0.8463   True Positive Rate: 61.7% True Negative Rate: 71.7%\n","Epoch: 29  D Loss: 1.29707  G Loss: 0.844724 True Positive Rate: 70.0% True Negative Rate: 65.0%\n","Epoch: 30  D Loss: 1.29421  G Loss: 0.85187  True Positive Rate: 40.0% True Negative Rate: 80.0%\n","Epoch: 31  D Loss: 1.29107  G Loss: 0.848582 True Positive Rate: 51.7% True Negative Rate: 73.3%\n","Epoch: 32  D Loss: 1.29467  G Loss: 0.847119 True Positive Rate: 68.3% True Negative Rate: 48.3%\n","Epoch: 33  D Loss: 1.2913   G Loss: 0.854978 True Positive Rate: 55.0% True Negative Rate: 60.0%\n","Epoch: 34  D Loss: 1.29719  G Loss: 0.845909 True Positive Rate: 55.0% True Negative Rate: 70.0%\n","Epoch: 35  D Loss: 1.28971  G Loss: 0.852627 True Positive Rate: 68.3% True Negative Rate: 60.0%\n","Epoch: 36  D Loss: 1.29121  G Loss: 0.849655 True Positive Rate: 40.0% True Negative Rate: 60.0%\n","Epoch: 37  D Loss: 1.29379  G Loss: 0.846983 True Positive Rate: 58.3% True Negative Rate: 68.3%\n","Epoch: 38  D Loss: 1.29     G Loss: 0.852708 True Positive Rate: 65.0% True Negative Rate: 73.3%\n","Epoch: 39  D Loss: 1.28942  G Loss: 0.855159 True Positive Rate: 61.7% True Negative Rate: 60.0%\n","Epoch: 40  D Loss: 1.28739  G Loss: 0.859351 True Positive Rate: 60.0% True Negative Rate: 55.0%\n","Epoch: 41  D Loss: 1.28749  G Loss: 0.857036 True Positive Rate: 63.3% True Negative Rate: 58.3%\n","Epoch: 42  D Loss: 1.28879  G Loss: 0.856052 True Positive Rate: 43.3% True Negative Rate: 68.3%\n","Epoch: 43  D Loss: 1.28852  G Loss: 0.854119 True Positive Rate: 58.3% True Negative Rate: 65.0%\n","Epoch: 44  D Loss: 1.28724  G Loss: 0.859693 True Positive Rate: 50.0% True Negative Rate: 73.3%\n","Epoch: 45  D Loss: 1.28439  G Loss: 0.866572 True Positive Rate: 61.7% True Negative Rate: 63.3%\n","Epoch: 46  D Loss: 1.28838  G Loss: 0.859093 True Positive Rate: 63.3% True Negative Rate: 51.7%\n","Epoch: 47  D Loss: 1.28905  G Loss: 0.852952 True Positive Rate: 58.3% True Negative Rate: 68.3%\n","Epoch: 48  D Loss: 1.29008  G Loss: 0.855745 True Positive Rate: 58.3% True Negative Rate: 70.0%\n","Epoch: 49  D Loss: 1.28838  G Loss: 0.859423 True Positive Rate: 58.3% True Negative Rate: 70.0%\n","Epoch: 50  D Loss: 1.28418  G Loss: 0.865382 True Positive Rate: 51.7% True Negative Rate: 73.3%\n","Epoch: 51  D Loss: 1.28472  G Loss: 0.857992 True Positive Rate: 51.7% True Negative Rate: 65.0%\n","Epoch: 52  D Loss: 1.28749  G Loss: 0.860667 True Positive Rate: 53.3% True Negative Rate: 61.7%\n","Epoch: 53  D Loss: 1.28773  G Loss: 0.860977 True Positive Rate: 65.0% True Negative Rate: 65.0%\n","Epoch: 54  D Loss: 1.28502  G Loss: 0.858876 True Positive Rate: 66.7% True Negative Rate: 65.0%\n","Epoch: 55  D Loss: 1.28326  G Loss: 0.861106 True Positive Rate: 60.0% True Negative Rate: 55.0%\n","Epoch: 56  D Loss: 1.2841   G Loss: 0.867487 True Positive Rate: 60.0% True Negative Rate: 73.3%\n","Epoch: 57  D Loss: 1.28269  G Loss: 0.867354 True Positive Rate: 68.3% True Negative Rate: 68.3%\n","Epoch: 58  D Loss: 1.2865   G Loss: 0.858417 True Positive Rate: 56.7% True Negative Rate: 70.0%\n","Epoch: 59  D Loss: 1.28536  G Loss: 0.863669 True Positive Rate: 65.0% True Negative Rate: 61.7%\n","Epoch: 60  D Loss: 1.28457  G Loss: 0.866035 True Positive Rate: 56.7% True Negative Rate: 70.0%\n","Epoch: 61  D Loss: 1.28189  G Loss: 0.868037 True Positive Rate: 73.3% True Negative Rate: 68.3%\n","Epoch: 62  D Loss: 1.28225  G Loss: 0.869222 True Positive Rate: 51.7% True Negative Rate: 60.0%\n","Epoch: 63  D Loss: 1.28071  G Loss: 0.866888 True Positive Rate: 63.3% True Negative Rate: 80.0%\n","Epoch: 64  D Loss: 1.28061  G Loss: 0.86873  True Positive Rate: 55.0% True Negative Rate: 58.3%\n","Epoch: 65  D Loss: 1.27676  G Loss: 0.876193 True Positive Rate: 53.3% True Negative Rate: 60.0%\n","Epoch: 66  D Loss: 1.28203  G Loss: 0.867817 True Positive Rate: 48.3% True Negative Rate: 58.3%\n","Epoch: 67  D Loss: 1.28362  G Loss: 0.869537 True Positive Rate: 60.0% True Negative Rate: 60.0%\n","Epoch: 68  D Loss: 1.2832   G Loss: 0.868702 True Positive Rate: 66.7% True Negative Rate: 68.3%\n","Epoch: 69  D Loss: 1.27986  G Loss: 0.870339 True Positive Rate: 61.7% True Negative Rate: 63.3%\n","Epoch: 70  D Loss: 1.28437  G Loss: 0.867909 True Positive Rate: 53.3% True Negative Rate: 75.0%\n","Epoch: 71  D Loss: 1.28     G Loss: 0.872519 True Positive Rate: 56.7% True Negative Rate: 51.7%\n","Epoch: 72  D Loss: 1.27907  G Loss: 0.87575  True Positive Rate: 70.0% True Negative Rate: 81.7%\n","Epoch: 73  D Loss: 1.28042  G Loss: 0.873787 True Positive Rate: 58.3% True Negative Rate: 61.7%\n","Epoch: 74  D Loss: 1.27471  G Loss: 0.879901 True Positive Rate: 61.7% True Negative Rate: 68.3%\n","Epoch: 75  D Loss: 1.27541  G Loss: 0.878734 True Positive Rate: 53.3% True Negative Rate: 75.0%\n","Epoch: 76  D Loss: 1.27957  G Loss: 0.875591 True Positive Rate: 56.7% True Negative Rate: 51.7%\n","Epoch: 77  D Loss: 1.27647  G Loss: 0.881347 True Positive Rate: 51.7% True Negative Rate: 65.0%\n","Epoch: 78  D Loss: 1.27972  G Loss: 0.871875 True Positive Rate: 65.0% True Negative Rate: 65.0%\n","Epoch: 79  D Loss: 1.27505  G Loss: 0.878769 True Positive Rate: 55.0% True Negative Rate: 75.0%\n","Epoch: 80  D Loss: 1.27561  G Loss: 0.879759 True Positive Rate: 48.3% True Negative Rate: 65.0%\n","Epoch: 81  D Loss: 1.27757  G Loss: 0.876944 True Positive Rate: 66.7% True Negative Rate: 81.7%\n","Epoch: 82  D Loss: 1.2727   G Loss: 0.880679 True Positive Rate: 45.0% True Negative Rate: 66.7%\n","Epoch: 83  D Loss: 1.27377  G Loss: 0.884236 True Positive Rate: 48.3% True Negative Rate: 70.0%\n","Epoch: 84  D Loss: 1.27543  G Loss: 0.879843 True Positive Rate: 73.3% True Negative Rate: 70.0%\n","Epoch: 85  D Loss: 1.27262  G Loss: 0.883837 True Positive Rate: 53.3% True Negative Rate: 61.7%\n","Epoch: 86  D Loss: 1.27557  G Loss: 0.881245 True Positive Rate: 46.7% True Negative Rate: 71.7%\n","Epoch: 87  D Loss: 1.27354  G Loss: 0.888755 True Positive Rate: 68.3% True Negative Rate: 71.7%\n","Epoch: 88  D Loss: 1.27591  G Loss: 0.882658 True Positive Rate: 51.7% True Negative Rate: 53.3%\n","Epoch: 89  D Loss: 1.27543  G Loss: 0.880732 True Positive Rate: 80.0% True Negative Rate: 65.0%\n","Epoch: 90  D Loss: 1.27581  G Loss: 0.878049 True Positive Rate: 50.0% True Negative Rate: 78.3%\n","Epoch: 91  D Loss: 1.27229  G Loss: 0.882451 True Positive Rate: 55.0% True Negative Rate: 70.0%\n","Epoch: 92  D Loss: 1.27369  G Loss: 0.884049 True Positive Rate: 56.7% True Negative Rate: 78.3%\n","Epoch: 93  D Loss: 1.27024  G Loss: 0.886449 True Positive Rate: 56.7% True Negative Rate: 75.0%\n","Epoch: 94  D Loss: 1.27359  G Loss: 0.885341 True Positive Rate: 63.3% True Negative Rate: 61.7%\n","Epoch: 95  D Loss: 1.27411  G Loss: 0.879282 True Positive Rate: 51.7% True Negative Rate: 80.0%\n","Epoch: 96  D Loss: 1.27526  G Loss: 0.879899 True Positive Rate: 55.0% True Negative Rate: 55.0%\n","Epoch: 97  D Loss: 1.27033  G Loss: 0.893547 True Positive Rate: 60.0% True Negative Rate: 80.0%\n","Epoch: 98  D Loss: 1.27808  G Loss: 0.879218 True Positive Rate: 43.3% True Negative Rate: 71.7%\n","Epoch: 99  D Loss: 1.27266  G Loss: 0.882205 True Positive Rate: 70.0% True Negative Rate: 70.0%\n","/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n","  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n","end_time :  1481.5514345169067\n","~~~"]},{"cell_type":"code","metadata":{"id":"w9qeGSgpuXpe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}